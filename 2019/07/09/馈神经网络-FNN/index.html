<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这两天在学习深度学习的相关知识，为了巩固自己所学的知识，在这里继续整理一下前馈神经网络.在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，可用一个有用无环图表示。前馈神经网络也成为多层感知器（Mutlti-Layer Perceptr">
<meta property="og:type" content="article">
<meta property="og:title" content="前馈神经网络-FNN">
<meta property="og:url" content="http://yoursite.com/2019/07/09/%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/index.html">
<meta property="og:site_name" content="DAWN">
<meta property="og:description" content="这两天在学习深度学习的相关知识，为了巩固自己所学的知识，在这里继续整理一下前馈神经网络.在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，可用一个有用无环图表示。前馈神经网络也成为多层感知器（Mutlti-Layer Perceptr">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-07-09T03:30:00.000Z">
<meta property="article:modified_time" content="2020-04-05T10:15:47.035Z">
<meta property="article:author" content="DCDC">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2019/07/09/%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>前馈神经网络-FNN | DAWN</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DAWN</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/09/%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="DCDC">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DAWN">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          前馈神经网络-FNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-07-09 11:30:00" itemprop="dateCreated datePublished" datetime="2019-07-09T11:30:00+08:00">2019-07-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2019/07/09/%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/09/馈神经网络-FNN/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这两天在学习深度学习的相关知识，为了巩固自己所学的知识，在这里继续整理一下前馈神经网络.<br>在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，<br>可用一个有用无环图表示。前馈神经网络也成为多层感知器（Mutlti-Layer Perceptron，MLP）。但是多层感知器的叫法并不准确，因为前馈神经网络其实是由多层Logistic回归模型（连续的非线性模型）组成，<br>而不是有多层感知器模型（非连续的非线性模型）组成</p>
<a id="more"></a> 
<h1 id="前馈神经网络结构及前向传播"><a href="#前馈神经网络结构及前向传播" class="headerlink" title="前馈神经网络结构及前向传播"></a>前馈神经网络结构及前向传播</h1><p>训练数据集</p>
<script type="math/tex; mode=display">\begin{align*} \\& T = \left\{ \left( \mathbf{x}_{1}, y_{1} \right), \left( \mathbf{x}_{2}, y_{2} \right), \cdots,
 \left(\mathbf{x}_i,y_i\right),\cdots,\left( \mathbf{x}_{N}, y_{N} \right) \right\} \end{align*}</script><p>其中，<script type="math/tex">\mathbf{x}_{i}</script>为第$i$个特征向量（实例），<script type="math/tex">\mathbf{x}_{i}=\left( x^{\left(1\right)}_i,x^{\left(2\right)}_i,\ldots
 ,x^{\left(j\right)}_i,\ldots ,x^{\left(n\right)}_i\right) ^{T} \in \mathcal{X} \subseteq \mathbb{R}^{n}</script>；<script type="math/tex">y_{i}</script>为<script type="math/tex">\mathbf{x}_{i}</script>的类别标<br> 记，类别标记表示为类别位置为1，其余位置为0的类别向量（one-hot编码）,$\mathbf{y}_i\in{0,1}^m$。</p>
<p>前馈神经网络输入层（层 1）</p>
<script type="math/tex; mode=display">\begin{align*} & \mathbf{a}^{1}=\left( a_{1}^{1},a_{2}^{1},\ldots ,a_{j}^{1},\ldots ,a_{n}^{1}\right) ^{T}\\ & a_{j}^{1}=x^{\left(j\right)}_i\quad\left( j=1,2,\ldots ,n\right) \end{align*}</script><p>前馈神经网络隐藏层（层 2）</p>
<script type="math/tex; mode=display">\begin{align*} & \mathbf{a}^{2}=\left( a_{1}^{2},a_{2}^{2},\ldots ,a_{j}^{2},\ldots  ,a_{p}^{2}\right) ^{T}\\ & a_{j}^{2}=\sigma \left( z_{j}^{2}\right) \\ & z_{j}^{2}= \sum _{k}w_{jk}^{2}\cdot a_{k}^{1}+b_{j}^{2}\quad\left( j=1,2,\ldots ,p\right) \\& \mathbf{z}^{2}=\left( z_{1}^{2},z_{2}^{2},\ldots ,z_{j}^{2},\ldots ,z_{p}^{2}\right) ^{T}\end{align*}</script><p>前馈神经网络输出层（层 3） </p>
<script type="math/tex; mode=display">\begin{align*} & \mathbf{a}^{3}=\left( a_{1}^{3},a_{2}^{3},\ldots ,a_{j}^{3},\ldots,a_{m}^{3}\right) ^{T}\\ & a_{j}^{3}=\sigma \left( z_{j}^{3}\right) \\ & z_{j}^{3}= \sum _{k}w_{jk}^{3}\cdot a_{k}^{2}+b_{j}^{3}\quad\left( j=1,2,\ldots ,m\right) \\& \mathbf{z}^{3}=\left( z_{1}^{3},z_{2}^{3},\ldots ,z_{j}^{3},\ldots ,z_{m}^{3}\right) ^{T}\end{align*}</script><p>预测输出 </p>
<script type="math/tex; mode=display">\begin{align*} & \hat{\mathbf{y}}=\left( \hat y_{1},\hat y_{2},\ldots ,\hat y_{j},\ldots ,\hat y_{m}\right) ^{T}\\ & \hat y_{j}=a_{j}^{3}\quad\left( j=1,2,\ldots ,m\right)\end{align*}</script><p>实际输出 </p>
<script type="math/tex; mode=display">\begin{align*} & \mathbf{y}=\left( y_{1},y_{2},\ldots ,y_{j},\ldots ,y_{m}\right) ^{T}  \quad\left( j=1,2,\ldots ,m\right) \end{align*}</script><p>其中，$\sigma\left(\cdot\right)$为激活函数。</p>
<h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>单个实例$\mathbf{x}$的损失函数$C_{\mathbf{x}}\left(\mathbf{y},\hat{\mathbf{y}}\right)$为平方损失函数、</p>
<script type="math/tex; mode=display">\begin{align*} & C_{\mathbf{x}}=\dfrac {1} {2}\left\| \mathbf{y}-\hat {\mathbf{y}}\right\| ^{2}=\dfrac {1} {2}\sum _{j}\left( y_{j}-\hat {y}_{j}\right) ^{2} \end{align*}</script><p>目标函数（经验风险）  <script type="math/tex">\begin{align*} & C=\dfrac {1} {N}\sum _{\mathbf{x}}C_{\mathbf{x}} \end{align*}</script><br>第$l$层参数$w_{jk}^l$更新为  </p>
<script type="math/tex; mode=display">\begin{align}
w_{jk}^l & \leftarrow w_{jk}^l-\alpha \dfrac {\partial C} {\partial w_{jk}^{l}}  \\
&=w_{jk}^l-\alpha\frac{1}{N}\sum_{\mathbf{x}}\dfrac {\partial C_x} {\partial w_{jk}^{l}}
\end{align}</script><p>第$l$层参数$b_j^l$更新为    </p>
<script type="math/tex; mode=display">\begin{align}
b_j^l & \leftarrow b_j^l-\alpha \dfrac {\partial C} {\partial b_j^{l}}  \\
&=b_j^l-\alpha\frac{1}{N}\sum_{\mathbf{x}}\dfrac {\partial C_x} {\partial b_j^{l}}
\end{align}</script><h1 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h1><p>定义第$l$层的第$j$个神经元上的误差 <script type="math/tex">\begin{align*} & \delta _{j}^{l}\equiv \dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}} \quad\left( l=2,3\right)\end{align*}</script><br>输出层误差 </p>
<script type="math/tex; mode=display">\begin{align} \delta _{j}^{3}&=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{3}} \\ 
& =\dfrac {\partial C_{\mathbf{x}}} {\partial a_{j}^{3}}\cdot\dfrac {\partial a_{j}^{3}} {\partial z_{j}^{3}} \\ & =\dfrac {\partial \left(\frac{1}{2}\sum_{j=1}^m \left(y_j-\hat{y}_j\right)^2\right)} {\partial \hat{y}_j}\cdot \sigma '\left( z_{j}^{3}\right) \\
& = \left(\hat{y}_j-y_j\right) \cdot \sigma'\left( z_{j}^{3} \right)\quad\left( j=1,2,\ldots ,m\right)\end{align}</script><p>隐藏层误差</p>
<script type="math/tex; mode=display">\begin{align} \delta _{j}^{2}&=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{2}}\\ 
& =\sum _{k}\dfrac {\partial C_{\mathbf{x}}} {\partial z_{k}^{3}}\cdot \dfrac {\partial z_{k}^{3}} {\partial z_{j}^{2}} \\ 
& = \sum _{k} \dfrac {\partial z_{k}^{3}} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ 
& = \sum _{k} \dfrac {\partial \left( \sum _{j}w_{kj}^{3}\cdot a_{j}^{2}+b_{k}^{3}\right)} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ 
& = \sum _{k} \dfrac {\partial \left( \sum _{j}w_{kj}^{3}\cdot  \sigma \left( z_{j}^{2}\right)+b_{k}^{3}\right)} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ 
& = \sum _{k} w_{kj}^{3}\cdot \sigma '\left( z_{j}^{2}\right) \cdot\delta _{k}^{3} \\ 
& = \sigma '\left( z_{j}^{2}\right) \cdot\sum _{k} w_{kj}^{3} \delta _{k}^{3} \quad\left( j=1,2,\ldots ,p\right)\quad\left( k=1,2,\ldots ,m\right)
\end{align}</script><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>损失函数在隐藏层（层2）／输出层（层3）关于偏置的梯度 </p>
<script type="math/tex; mode=display">\begin{align*} & \dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}}\cdot \dfrac {\partial z_{j}^{l}} {\partial b_{j}^{l}}=\delta _{j}^{l}\cdot \dfrac {\partial \left( \sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\right) } {\partial b_{j}^{l}}=\delta _{j}^{l}\quad\left( l=2,3\right)\end{align*}</script><p>损失函数在隐藏层（层2）／输出层（层3）关于权值的梯度</p>
<script type="math/tex; mode=display">\begin{align*} & \dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}}\cdot \dfrac {\partial z_{j}^{l}} {\partial w_{jk}^{l}}=\delta _{j}^{l}\cdot \dfrac {\partial \left( \sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\right) } {\partial w_{jk}^{l}}=\delta _{j}^{l}\cdot a_{k}^{l-1}\quad\left( l=2,3\right)\end{align*}</script><p>偏置与权值的梯度计算算法<br>输入：实例$\mathbf{x}=\left(x^{\left(1\right)},\cdots,x^{\left(n\right)}\right)^\top$<br>输出：损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度</p>
<script type="math/tex; mode=display">\left(\dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}\right)和\left(\dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}\right)</script><ol>
<li>为输入层设置对应的激活值<script type="math/tex">\left(a_j^1\right)_{j=1}^n=\left(x^{\left(j\right)}\right)_{j=1}^n</script></li>
<li>前向传播：对每个$l（l=2,3）$计算<script type="math/tex">\begin{align*} &a_{j}^{l}=\sigma \left( z_{j}^{l}\right) \\ & z_{j}^{l}= \sum _{k}w_{jk}^{l}\cdot a_{k}^{l-1}+b_{j}^{l}\end{align*}</script></li>
<li>计算输出层误差<script type="math/tex">\left(\delta _{j}^{3}\right)_{j=1}^m</script>；  </li>
<li>计算误差反向传播：隐藏层误差<script type="math/tex">\left(\delta _{j}^{2}\right)_{j=1}^p</script>；  </li>
<li>计算损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度<script type="math/tex">\left(\dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}\right)$和$\left(\dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}\right)</script>。</li>
</ol>
<h1 id="前馈神经网络的改进"><a href="#前馈神经网络的改进" class="headerlink" title="前馈神经网络的改进"></a>前馈神经网络的改进</h1><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>单个实例$\mathbf{x}$的损失函数$C_{\mathbf{x}}$为交叉熵损失函数</p>
<script type="math/tex; mode=display">C_{\mathbf{x}}=-\sum _{j}\left[y_j\log\hat{y}_j+\left(1-y_j\right)\log\left(1-\hat{y}_j\right)\right]</script><h2 id="正则化目标函数"><a href="#正则化目标函数" class="headerlink" title="正则化目标函数"></a>正则化目标函数</h2><p>$L_2$正则化的目标函数（结构风险） </p>
<script type="math/tex; mode=display">C=\frac{1}{N}\left(\sum _{\mathbf{x}}C_{\mathbf{x}}+ \frac{\lambda}{2}\sum_w w^2\right)</script><h2 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h2><p>设对$l$层有有<script type="math/tex">n_{in}^l</script>个输入神经元，使用均值为$0$，标准差为<script type="math/tex">\frac{1}{\sqrt{n_{in}^l}}</script>的高斯分布初始化$l$层的权值</p>
<script type="math/tex; mode=display">w^l\sim N\left(0,\frac{1}{n_{in}^l}\right)</script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2019/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/" rel="next" title="机器学习模型评估方法">
      机器学习模型评估方法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前馈神经网络结构及前向传播"><span class="nav-number">1.</span> <span class="nav-text">前馈神经网络结构及前向传播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参数学习"><span class="nav-number">2.</span> <span class="nav-text">参数学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#误差反向传播"><span class="nav-number">3.</span> <span class="nav-text">误差反向传播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降算法"><span class="nav-number">4.</span> <span class="nav-text">梯度下降算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前馈神经网络的改进"><span class="nav-number">5.</span> <span class="nav-text">前馈神经网络的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵损失函数"><span class="nav-number">5.1.</span> <span class="nav-text">交叉熵损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化目标函数"><span class="nav-number">5.2.</span> <span class="nav-text">正则化目标函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#权值初始化"><span class="nav-number">5.3.</span> <span class="nav-text">权值初始化</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">DCDC</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DCDC</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://dongchao.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://yoursite.com/2019/07/09/%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-FNN/";
    this.page.identifier = "2019/07/09/馈神经网络-FNN/";
    this.page.title = "前馈神经网络-FNN";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://dongchao.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
